{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "install the model"
      ],
      "metadata": {
        "id": "ddoZz0UjOC5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer.git"
      ],
      "metadata": {
        "id": "wHhWRHh14pVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "jSliW29046PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/bark-voice-cloning-HuBERT-quantizer\n",
        "!ls\n",
        "from hubert.hubert_manager import *\n",
        "\n",
        "HuBERTManager.make_sure_hubert_installed()\n",
        "print(\"hubert_installed\")\n",
        "HuBERTManager.make_sure_tokenizer_installed()\n",
        "print(\"tokenizer_installed\")"
      ],
      "metadata": {
        "id": "cD4r3EPz4xVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio torch encodec numpy fairseq audiolm_pytorch tensorboardX funcy"
      ],
      "metadata": {
        "id": "aG7p7MvP64-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add your file(as a wav), rename it to \"audio.wav\", and make sure it is in /content (same directory with config and sample_data)\n",
        "\n",
        "Its recommended to shorten the audio file to 15-20 seconds, and to use the end, not the beginning\n",
        "\n",
        "# add your audio now"
      ],
      "metadata": {
        "id": "pc72ur8xUp5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hubert.pre_kmeans_hubert import CustomHubert\n",
        "import torchaudio\n",
        "\n",
        "# Load the HuBERT model,\n",
        "# checkpoint_path should work fine with data/models/hubert/hubert.pt for the default config\n",
        "hubert_model = CustomHubert(checkpoint_path='data/models/hubert/hubert.pt')\n",
        "\n",
        "# Run the model to extract semantic features from an audio file, where wav is your audio file\n",
        "wav, sr = torchaudio.load('/content/audio.wav') # This is where you load your wav, with soundfile or torchaudio for example\n",
        "\n",
        "if wav.shape[0] == 2:  # Stereo to mono if needed\n",
        "    wav = wav.mean(0, keepdim=True)\n",
        "\n",
        "semantic_vectors = hubert_model.forward(wav, input_sample_hz=sr)"
      ],
      "metadata": {
        "id": "zFOBdlTF-2YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from hubert.customtokenizer import CustomTokenizer\n",
        "\n",
        "# Load the CustomTokenizer model from a checkpoint\n",
        "# With default config, you can use the pretrained model from huggingface\n",
        "# With the default setup from HuBERTManager, this will be in data/models/hubert/tokenizer.pth\n",
        "tokenizer = CustomTokenizer.load_from_checkpoint('/content/bark-voice-cloning-HuBERT-quantizer/data/models/hubert/tokenizer.pth')  # Automatically uses the right layers\n",
        "\n",
        "# Process the semantic vectors from the previous HuBERT run (This works in batches, so you can send the entire HuBERT output)\n",
        "semantic_tokens = tokenizer.get_token(semantic_vectors)\n",
        "\n",
        "# Congratulations! You now have semantic tokens which can be used inside of a speaker prompt file.\n"
      ],
      "metadata": {
        "id": "krzCOJZpEmyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from encodec import EncodecModel\n",
        "from encodec.utils import convert_audio\n",
        "\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Instantiate a pretrained EnCodec model\n",
        "model = EncodecModel.encodec_model_24khz()\n",
        "# The number of codebooks used will be determined bythe bandwidth selected.\n",
        "# E.g. for a bandwidth of 6kbps, `n_q = 8` codebooks are used.\n",
        "# Supported bandwidths are 1.5kbps (n_q = 2), 3 kbps (n_q = 4), 6 kbps (n_q = 8) and 12 kbps (n_q =16) and 24kbps (n_q=32).\n",
        "# For the 48 kHz model, only 3, 6, 12, and 24 kbps are supported. The number\n",
        "# of codebooks for each is half that of the 24 kHz model as the frame rate is twice as much.\n",
        "model.set_target_bandwidth(6.0)\n",
        "\n",
        "# Load and pre-process the audio waveform\n",
        "wav, sr = torchaudio.load(\"/content/audio.wav\")\n",
        "wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n",
        "wav = wav.unsqueeze(0)\n",
        "\n",
        "# Extract discrete codes from EnCodec\n",
        "with torch.no_grad():\n",
        "    encoded_frames = model.encode(wav)\n",
        "codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()\n"
      ],
      "metadata": {
        "id": "bnncycu5EppG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy\n",
        "\n",
        "fine_prompt = codes\n",
        "\n",
        "coarse_prompt = fine_prompt[:2, :]\n",
        "\n",
        "semantics = semantic_tokens\n",
        "\n",
        "numpy.savez(semantic_prompt=semantics, fine_prompt=fine_prompt, coarse_prompt=coarse_prompt, file=\"helloWorld.npz\")"
      ],
      "metadata": {
        "id": "rsFzD2fTEtmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that we have the voice cloned as an npz, we can make text to speech with it!"
      ],
      "metadata": {
        "id": "xUgtFe9XnmRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/suno-ai/bark.git\n",
        "%cd bark"
      ],
      "metadata": {
        "id": "QQk0LTdqRmLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "from IPython.display import Audio\n",
        "\n",
        "# download and load all models\n",
        "preload_models()\n",
        "\n",
        "# generate audio from text\n",
        "text_prompt = \"\"\"\n",
        "     Yay! Working voice cloning!\n",
        "\"\"\"\n",
        "audio_array = generate_audio(text=text_prompt, history_prompt='/content/bark-voice-cloning-HuBERT-quantizer/helloWorld.npz')\n",
        "\n",
        "# save audio to disk\n",
        "write_wav(\"bark_generation.wav\", SAMPLE_RATE, audio_array)\n",
        "  \n",
        "# play text in notebook\n",
        "Audio(audio_array, rate=SAMPLE_RATE)"
      ],
      "metadata": {
        "id": "bRvO6RstRpMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}